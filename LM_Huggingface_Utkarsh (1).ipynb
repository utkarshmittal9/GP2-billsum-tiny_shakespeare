{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM_Huggingface_Utkarsh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b629e5a8143a43918f2674342162d5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8eaa8ebce79f47b7a3240e1a6b09f0eb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6623347d31964d9bb20d22fe33044193",
              "IPY_MODEL_69209cc71d83422abb99da1c0715ae1b"
            ]
          }
        },
        "8eaa8ebce79f47b7a3240e1a6b09f0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6623347d31964d9bb20d22fe33044193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22531f411d2d46d798f90592b903d654",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8398da0c00d4473b434577a7f9af3b7"
          }
        },
        "69209cc71d83422abb99da1c0715ae1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e444c5f7e8cc4a8797ad30406ce9e982",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  2.94 url/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c21933470c784e22879b9cd1c77f4578"
          }
        },
        "22531f411d2d46d798f90592b903d654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8398da0c00d4473b434577a7f9af3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e444c5f7e8cc4a8797ad30406ce9e982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c21933470c784e22879b9cd1c77f4578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56f5c8388f5e407da1c144b847622fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2b827d38e0bc439dbc2fc32a717eb5d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d217cf5df4a145cabc4d887cd9ae0ec9",
              "IPY_MODEL_df1b7b7ab8674de6bdacfede5808e624"
            ]
          }
        },
        "2b827d38e0bc439dbc2fc32a717eb5d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d217cf5df4a145cabc4d887cd9ae0ec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5037e188d654d748cecd6095b9aac2d",
            "_dom_classes": [],
            "description": "Dl Size...: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53386468207e4897a20e4dd6d0d2dd8a"
          }
        },
        "df1b7b7ab8674de6bdacfede5808e624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40c780c769cb4653b79dc0f78b3b84cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/0 [00:00&lt;00:00,  3.17 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1137cc76bcaf4b6882c284761ff5f2d9"
          }
        },
        "a5037e188d654d748cecd6095b9aac2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53386468207e4897a20e4dd6d0d2dd8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40c780c769cb4653b79dc0f78b3b84cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1137cc76bcaf4b6882c284761ff5f2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fb02008d8884691af0d12e12b4fb18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8c74de228a0f4fc0867ae6161dee99be",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89a1d886fb504394876eb7de833f1a51",
              "IPY_MODEL_147b42ed0d2b4a3eacf1e0d06771c541"
            ]
          }
        },
        "8c74de228a0f4fc0867ae6161dee99be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89a1d886fb504394876eb7de833f1a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_993c43c4581b450296207b313782ecd5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0799d9573bed4000b440d025897eaee4"
          }
        },
        "147b42ed0d2b4a3eacf1e0d06771c541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3413756ebeca4f1e8b1113c99e8fe756",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/0 [00:00&lt;00:00, 43.55 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e892f36a73a3451a9dd21651924c42eb"
          }
        },
        "993c43c4581b450296207b313782ecd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0799d9573bed4000b440d025897eaee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3413756ebeca4f1e8b1113c99e8fe756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e892f36a73a3451a9dd21651924c42eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27301d155a2145418b41ed278fb34636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_116d263d9b14487f94ac668f75be56a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c4eeca3ff3b4b358ec79c6ef158da33",
              "IPY_MODEL_842b5677569a41c28200a5e31cc027bd"
            ]
          }
        },
        "116d263d9b14487f94ac668f75be56a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c4eeca3ff3b4b358ec79c6ef158da33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba0525072ded49a7b60cd9e55722e60b",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd553180b7bf4b70b96c26aaced8b1c6"
          }
        },
        "842b5677569a41c28200a5e31cc027bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b2e6b3e023724c40b757244df17aab34",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1 [00:00&lt;?, ? examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3e0fb1586a94d039b80093e7a9330f7"
          }
        },
        "ba0525072ded49a7b60cd9e55722e60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd553180b7bf4b70b96c26aaced8b1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2e6b3e023724c40b757244df17aab34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3e0fb1586a94d039b80093e7a9330f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "766e4a765fa24cfd9c74c515964cae0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e4a84bd33094eb683fb07a1fa115111",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3722114583894eb992a8b2d17c473461",
              "IPY_MODEL_980c54d514ab4894adfdffd1d120b335"
            ]
          }
        },
        "8e4a84bd33094eb683fb07a1fa115111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3722114583894eb992a8b2d17c473461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e673f5d2cdab4a80a06db0a9dd726aa6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5e762f4cc0f48ebbe895d9f5e0f7d11"
          }
        },
        "980c54d514ab4894adfdffd1d120b335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3304e56fd979444888ab0403268a9d34",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/0 [00:00&lt;00:00, 37.55 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ce532d727894eeb9574d976d4975a60"
          }
        },
        "e673f5d2cdab4a80a06db0a9dd726aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5e762f4cc0f48ebbe895d9f5e0f7d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3304e56fd979444888ab0403268a9d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ce532d727894eeb9574d976d4975a60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d32549c6e184551953657dab58160d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6562b0397624306a4e00edcbc0ad33f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1cd2240fc7a344e49985d7c14c5feae0",
              "IPY_MODEL_75e43e6c92ed4f34b94bf57f819edbfa"
            ]
          }
        },
        "e6562b0397624306a4e00edcbc0ad33f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cd2240fc7a344e49985d7c14c5feae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2b5c521ca14c4002bea1eb1bded696a2",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9b308ca94134cebb318600c0922873d"
          }
        },
        "75e43e6c92ed4f34b94bf57f819edbfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca17d38f0ee640f8ad7fb67836dec6c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1 [00:00&lt;?, ? examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7c77b2b854641fe90455badce330aa7"
          }
        },
        "2b5c521ca14c4002bea1eb1bded696a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9b308ca94134cebb318600c0922873d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca17d38f0ee640f8ad7fb67836dec6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7c77b2b854641fe90455badce330aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b452489f2d62441a96fe99debab30b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c382d43a586c488794d3f0f10de7d9c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a78587a210ed42d5a8dc132d88a71308",
              "IPY_MODEL_b2252fe3e45c4ac4815554b7c7472b86"
            ]
          }
        },
        "c382d43a586c488794d3f0f10de7d9c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a78587a210ed42d5a8dc132d88a71308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50b2751721e34553aa159e2c82f4c491",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef77dbb8319f4500bb816183a4601ba3"
          }
        },
        "b2252fe3e45c4ac4815554b7c7472b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_86c7f41dd17e4c5896b5502c35b99f87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/0 [00:00&lt;00:00, 37.78 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_213ac8e6e5174406b17e388970c26556"
          }
        },
        "50b2751721e34553aa159e2c82f4c491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef77dbb8319f4500bb816183a4601ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86c7f41dd17e4c5896b5502c35b99f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "213ac8e6e5174406b17e388970c26556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48cf1a3ff50a4fdd82d4d3e2b2f073ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_52987558604949178429bce57a520327",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ed2ef4ce6e8b4899b125c1934ef746bf",
              "IPY_MODEL_5ff968b756004bb5a671cbf034200f1f"
            ]
          }
        },
        "52987558604949178429bce57a520327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed2ef4ce6e8b4899b125c1934ef746bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b17480829544f46b7eeeddea9ac4b49",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cee8d369ad8a45afbb7913e9f66c4b97"
          }
        },
        "5ff968b756004bb5a671cbf034200f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_819827a5980149389835e1199d314f66",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1 [00:00&lt;?, ? examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dbadffba9f0f4c968166fcc81939e6fa"
          }
        },
        "0b17480829544f46b7eeeddea9ac4b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cee8d369ad8a45afbb7913e9f66c4b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "819827a5980149389835e1199d314f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dbadffba9f0f4c968166fcc81939e6fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aKwrJ4uafhX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rmSAUQVIUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fdb2166-1ab1-495e-a94d-7052a9ed8735"
      },
      "source": [
        "#Clone the transformers repo into the notebook\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 62213 (delta 3), reused 9 (delta 0), pack-reused 62194\u001b[K\n",
            "Receiving objects: 100% (62213/62213), 47.42 MiB | 3.05 MiB/s, done.\n",
            "Resolving deltas: 100% (44028/44028), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_9TPC2IjDuL",
        "outputId": "c819b375-7016-4191-dc62-02ca36f6dfb0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\r\u001b[K     |▏                               | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 26.0MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 28.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 20.0MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 16.6MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 18.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 14.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 14.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92kB 14.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 13.2MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 13.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122kB 13.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 13.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 143kB 13.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153kB 13.2MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 13.2MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 13.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184kB 13.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 194kB 13.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 204kB 13.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215kB 13.2MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 13.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 235kB 13.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 245kB 13.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 256kB 13.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 266kB 13.2MB/s eta 0:00:01\r\u001b[K     |████▉                           | 276kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 286kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 296kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 307kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 317kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 327kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 337kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 348kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 358kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 368kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 378kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 389kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 399kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 409kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 419kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 430kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 440kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 450kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 460kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 471kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 481kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 491kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 501kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 512kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 522kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 532kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 542kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 552kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 563kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 573kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 583kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 593kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 604kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 614kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 624kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 634kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 645kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 655kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 665kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 675kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 686kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 696kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 706kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 716kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 727kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 737kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 747kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 757kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 768kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 778kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 788kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 798kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 808kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 819kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 829kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 839kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 849kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 860kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 870kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 880kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 890kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 901kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 911kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 921kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 931kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 942kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 952kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 962kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 972kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 983kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 993kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.0MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.0MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.7MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.8MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.8MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.8MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.8MB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.8MB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f87fc77d2e80c6bfc988d2084b4e880b143b0a262b8f43355e64c301616c1698\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFzEr4Y1VJKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac86a19c-b56b-472b-cb8f-5538515749ed"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-ke4920tDqv"
      },
      "source": [
        "Check to see what gpu we were granted. For Colab Pro it will vary between a Tesla V100 or P100. For normal colab it should be a k80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPIgByLqmI81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05435f6-1b65-45a3-cad3-a3aa6fa1c36e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 10 14:00:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2M2Oz9CYB4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdf440a-ba90-4ab1-b90d-61264483818d"
      },
      "source": [
        "import os\n",
        "os.chdir(\"transformers\")\n",
        "os.chdir(\"./examples/language-modeling\")\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md\t  run_clm.py\t   run_mlm.py\n",
            "requirements.txt  run_mlm_flax.py  run_plm.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04rBGxwiYnep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871d1bc7-d426-4cf1-e186-3e3a34ae1834"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets>=1.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 13.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 27.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/67/2f4fcce1b41bcc7e88a6bfdb42046597ae72e5bc95c2789b7c5ac893c433/pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7MB)\n",
            "\u001b[K     |████████████████████████████████| 20.7MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->-r requirements.txt (line 3)) (53.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\n",
            "Installing collected packages: pyarrow, xxhash, datasets, sentencepiece\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.1 pyarrow-3.0.0 sentencepiece-0.1.95 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB29mAKjaNIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca592ae-96db-4bd3-bd34-cc99e8174efe"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md\t  run_clm.py\t   run_mlm.py\n",
            "requirements.txt  run_mlm_flax.py  run_plm.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo5gRmXaWx0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb78a6f-70ab-4530-b02a-568c899a7599"
      },
      "source": [
        "!pip install pyarrow --upgrade"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5sdYSpAWY1S"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/transformers/examples/\")\n",
        "os.chdir(\"./language-modeling\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6G6WINaYmwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db12e966-72e4-4289-d0af-c15dc3da8199"
      },
      "source": [
        "# Need to install latest transformer packages from github so the scripts will run correctly\n",
        "! pip install git+git://github.com/huggingface/transformers/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/huggingface/transformers/\n",
            "  Cloning git://github.com/huggingface/transformers/ to /tmp/pip-req-build-ol_kqq5v\n",
            "  Running command git clone -q git://github.com/huggingface/transformers/ /tmp/pip-req-build-ol_kqq5v\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.4.0.dev0-cp36-none-any.whl size=1808787 sha256=a6ac3d0199de5df9bfd7c0b656c820eca8fa7115f983622cb7cbab61a050e802\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b1htgsbz/wheels/dc/e5/1e/3a2977a646558fca07585cadcf56aa4a910e995ba945961c4e\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.3.2\n",
            "    Uninstalling transformers-4.3.2:\n",
            "      Successfully uninstalled transformers-4.3.2\n",
            "Successfully installed transformers-4.4.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDuJnVmrY_Fb"
      },
      "source": [
        "Set up data from a text file in the format <|title|> some data <|endoftext|> and split into training and eval sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK1drH0H98Py",
        "outputId": "5e7be5ee-8c15-4d0d-fda6-836a11d37827"
      },
      "source": [
        "ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md         \u001b[0m\u001b[01;32mrun_clm.py\u001b[0m*       \u001b[01;32mrun_mlm.py\u001b[0m*\n",
            "requirements.txt  \u001b[01;32mrun_mlm_flax.py\u001b[0m*  \u001b[01;32mrun_plm.py\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TYi8Oqs6Npo"
      },
      "source": [
        "The code below defines a function that imports tiny_shakespeare datas set and extract the text from it for training the GPT 2 model. We use this funciton to import text for training and test. Also, it saves the text file as train.text and test.txt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFVBXuTkat8d"
      },
      "source": [
        "# **Fine tuning GPT2 model on Bill Sum data set**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAcKt8cqMfxF"
      },
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "def to_txt_save(split_name):\n",
        "  d = tfds.load(name='tiny_shakespeare')[split_name]\n",
        "  d = list(d)[0]['text'].numpy().decode(\"utf-8\")\n",
        "  with open(split_name+'.txt', 'w') as f:\n",
        "    f.write(d)  \n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8-2_0hUxaaj",
        "outputId": "bb1ff602-4d28-408b-acfc-946c3e0acf4e"
      },
      "source": [
        "ls"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md         \u001b[0m\u001b[01;32mrun_clm.py\u001b[0m*       \u001b[01;32mrun_mlm.py\u001b[0m*\n",
            "requirements.txt  \u001b[01;32mrun_mlm_flax.py\u001b[0m*  \u001b[01;32mrun_plm.py\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yxYQJKiUesq"
      },
      "source": [
        "os.chdir(\"/content\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQCMzmgWxmGJ",
        "outputId": "77c69821-77a6-4e44-bf1e-4d02ef9f07a5"
      },
      "source": [
        "ls"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtransformers\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN0wmhfRSxTs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "b629e5a8143a43918f2674342162d5db",
            "8eaa8ebce79f47b7a3240e1a6b09f0eb",
            "6623347d31964d9bb20d22fe33044193",
            "69209cc71d83422abb99da1c0715ae1b",
            "22531f411d2d46d798f90592b903d654",
            "a8398da0c00d4473b434577a7f9af3b7",
            "e444c5f7e8cc4a8797ad30406ce9e982",
            "c21933470c784e22879b9cd1c77f4578",
            "56f5c8388f5e407da1c144b847622fc5",
            "2b827d38e0bc439dbc2fc32a717eb5d7",
            "d217cf5df4a145cabc4d887cd9ae0ec9",
            "df1b7b7ab8674de6bdacfede5808e624",
            "a5037e188d654d748cecd6095b9aac2d",
            "53386468207e4897a20e4dd6d0d2dd8a",
            "40c780c769cb4653b79dc0f78b3b84cd",
            "1137cc76bcaf4b6882c284761ff5f2d9",
            "7fb02008d8884691af0d12e12b4fb18e",
            "8c74de228a0f4fc0867ae6161dee99be",
            "89a1d886fb504394876eb7de833f1a51",
            "147b42ed0d2b4a3eacf1e0d06771c541",
            "993c43c4581b450296207b313782ecd5",
            "0799d9573bed4000b440d025897eaee4",
            "3413756ebeca4f1e8b1113c99e8fe756",
            "e892f36a73a3451a9dd21651924c42eb",
            "27301d155a2145418b41ed278fb34636",
            "116d263d9b14487f94ac668f75be56a7",
            "6c4eeca3ff3b4b358ec79c6ef158da33",
            "842b5677569a41c28200a5e31cc027bd",
            "ba0525072ded49a7b60cd9e55722e60b",
            "dd553180b7bf4b70b96c26aaced8b1c6",
            "b2e6b3e023724c40b757244df17aab34",
            "d3e0fb1586a94d039b80093e7a9330f7",
            "766e4a765fa24cfd9c74c515964cae0c",
            "8e4a84bd33094eb683fb07a1fa115111",
            "3722114583894eb992a8b2d17c473461",
            "980c54d514ab4894adfdffd1d120b335",
            "e673f5d2cdab4a80a06db0a9dd726aa6",
            "c5e762f4cc0f48ebbe895d9f5e0f7d11",
            "3304e56fd979444888ab0403268a9d34",
            "6ce532d727894eeb9574d976d4975a60",
            "7d32549c6e184551953657dab58160d4",
            "e6562b0397624306a4e00edcbc0ad33f",
            "1cd2240fc7a344e49985d7c14c5feae0",
            "75e43e6c92ed4f34b94bf57f819edbfa",
            "2b5c521ca14c4002bea1eb1bded696a2",
            "b9b308ca94134cebb318600c0922873d",
            "ca17d38f0ee640f8ad7fb67836dec6c4",
            "c7c77b2b854641fe90455badce330aa7",
            "b452489f2d62441a96fe99debab30b3a",
            "c382d43a586c488794d3f0f10de7d9c8",
            "a78587a210ed42d5a8dc132d88a71308",
            "b2252fe3e45c4ac4815554b7c7472b86",
            "50b2751721e34553aa159e2c82f4c491",
            "ef77dbb8319f4500bb816183a4601ba3",
            "86c7f41dd17e4c5896b5502c35b99f87",
            "213ac8e6e5174406b17e388970c26556",
            "48cf1a3ff50a4fdd82d4d3e2b2f073ff",
            "52987558604949178429bce57a520327",
            "ed2ef4ce6e8b4899b125c1934ef746bf",
            "5ff968b756004bb5a671cbf034200f1f",
            "0b17480829544f46b7eeeddea9ac4b49",
            "cee8d369ad8a45afbb7913e9f66c4b97",
            "819827a5980149389835e1199d314f66",
            "dbadffba9f0f4c968166fcc81939e6fa"
          ]
        },
        "outputId": "3fb14177-6969-4337-8f7e-4245790f210c"
      },
      "source": [
        "to_txt_save('train')#Downloading text for training using tiny_shakespeare dataset"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: tiny_shakespeare/1.0.0\n",
            "INFO:absl:Load dataset info from /tmp/tmpxxk8e0yqtfds\n",
            "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
            "INFO:absl:Generating dataset tiny_shakespeare (/root/tensorflow_datasets/tiny_shakespeare/1.0.0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset tiny_shakespeare/1.0.0 (download: Unknown size, generated: 1.06 MiB, total: 1.06 MiB) to /root/tensorflow_datasets/tiny_shakespeare/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b629e5a8143a43918f2674342162d5db",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56f5c8388f5e407da1c144b847622fc5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Downloading https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt into /root/tensorflow_datasets/downloads/raw.gith.com_karp_char-rnn_mast_tiny_inpugogO998CpE557gFI85J16FbtM1Ig3B0ySj9UhS6f7GM.txt.tmp.95ad34362cf94446bb0632f4676f4095...\n",
            "INFO:absl:Generating split train\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fb02008d8884691af0d12e12b4fb18e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/tiny_shakespeare/1.0.0.incompleteJQYO0U/tiny_shakespeare-train.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27301d155a2145418b41ed278fb34636",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Done writing /root/tensorflow_datasets/tiny_shakespeare/1.0.0.incompleteJQYO0U/tiny_shakespeare-train.tfrecord. Shard lengths: [1]\n",
            "INFO:absl:Generating split validation\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "766e4a765fa24cfd9c74c515964cae0c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/tiny_shakespeare/1.0.0.incompleteJQYO0U/tiny_shakespeare-validation.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d32549c6e184551953657dab58160d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Done writing /root/tensorflow_datasets/tiny_shakespeare/1.0.0.incompleteJQYO0U/tiny_shakespeare-validation.tfrecord. Shard lengths: [1]\n",
            "INFO:absl:Generating split test\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b452489f2d62441a96fe99debab30b3a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/tiny_shakespeare/1.0.0.incompleteJQYO0U/tiny_shakespeare-test.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48cf1a3ff50a4fdd82d4d3e2b2f073ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Done writing /root/tensorflow_datasets/tiny_shakespeare/1.0.0.incompleteJQYO0U/tiny_shakespeare-test.tfrecord. Shard lengths: [1]\n",
            "INFO:absl:Skipping computing stats for mode ComputeStatsMode.SKIP.\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/tiny_shakespeare/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r\u001b[1mDataset tiny_shakespeare downloaded and prepared to /root/tensorflow_datasets/tiny_shakespeare/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQQOcrShS2bE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2faa30fd-92f0-4fb6-a42c-18af07672d04"
      },
      "source": [
        "to_txt_save('test')##Downloading text for test/validation using tiny_shakespeare dataset"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/tiny_shakespeare/1.0.0\n",
            "INFO:absl:Reusing dataset tiny_shakespeare (/root/tensorflow_datasets/tiny_shakespeare/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/tiny_shakespeare/1.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltn2TpZsTPlX"
      },
      "source": [
        "#deleting the existing output folder if any. Run only when the model has a folder named output from previous run\n",
        "# !rm -rf /content/output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyV_rTL3ZE_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5eade26-f44d-4b8a-c45d-c476a16d95fb"
      },
      "source": [
        "#Fine tuning GPT2 model at Epoch 1 and learning rate 5e-5 \n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 07:16:18.773590: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/10/2021 07:16:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 07:16:20 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_07-16-19_d646f54afccc, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 2.75MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-c18b101ca8952b0d (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1302] 2021-02-10 07:16:20,403 >> https://huggingface.co/gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzb_xxbiq\n",
            "Downloading: 100% 718/718 [00:00<00:00, 720kB/s]\n",
            "[INFO|file_utils.py:1306] 2021-02-10 07:16:20,549 >> storing https://huggingface.co/gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|file_utils.py:1309] 2021-02-10 07:16:20,549 >> creating metadata file for /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 07:16:20,549 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 07:16:20,550 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 07:16:20,599 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 07:16:20,600 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1302] 2021-02-10 07:16:20,657 >> https://huggingface.co/gpt2-medium/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7w1kolnc\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 13.5MB/s]\n",
            "[INFO|file_utils.py:1306] 2021-02-10 07:16:20,791 >> storing https://huggingface.co/gpt2-medium/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1309] 2021-02-10 07:16:20,791 >> creating metadata file for /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1302] 2021-02-10 07:16:20,846 >> https://huggingface.co/gpt2-medium/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgl2y30vp\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 7.93MB/s]\n",
            "[INFO|file_utils.py:1306] 2021-02-10 07:16:20,956 >> storing https://huggingface.co/gpt2-medium/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1309] 2021-02-10 07:16:20,956 >> creating metadata file for /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1302] 2021-02-10 07:16:21,017 >> https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdy2bdpjv\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 15.3MB/s]\n",
            "[INFO|file_utils.py:1306] 2021-02-10 07:16:21,166 >> storing https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1309] 2021-02-10 07:16:21,166 >> creating metadata file for /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:16:21,166 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:16:21,166 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:16:21,166 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1302] 2021-02-10 07:16:21,280 >> https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsi2knh6t\n",
            "Downloading: 100% 1.52G/1.52G [00:16<00:00, 89.4MB/s]\n",
            "[INFO|file_utils.py:1306] 2021-02-10 07:16:38,491 >> storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|file_utils.py:1309] 2021-02-10 07:16:38,491 >> creating metadata file for /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 07:16:38,491 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 07:16:50,861 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 07:16:50,861 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 36/36 [00:00<00:00, 41.79ba/s]\n",
            "100% 3/3 [00:00<00:00, 109.89ba/s]\n",
            "100% 36/36 [00:01<00:00, 31.74ba/s]\n",
            "100% 3/3 [00:00<00:00, 46.25ba/s]\n",
            "[INFO|trainer.py:432] 2021-02-10 07:16:59,950 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-10 07:16:59,951 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-10 07:16:59,951 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:16:59,952 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:16:59,957 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-10 07:16:59,957 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-10 07:16:59,957 >>   Num examples = 245\n",
            "[INFO|trainer.py:839] 2021-02-10 07:16:59,957 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:840] 2021-02-10 07:16:59,957 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-10 07:16:59,958 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 07:16:59,958 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-10 07:16:59,958 >>   Total optimization steps = 245\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:16:59,963 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/245 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "100% 245/245 [02:41<00:00,  1.52it/s][INFO|trainer.py:1007] 2021-02-10 07:19:41,116 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.1589, 'train_samples_per_second': 1.52, 'epoch': 1.0}\n",
            "100% 245/245 [02:41<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:1408] 2021-02-10 07:19:41,118 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-02-10 07:19:41,120 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 07:19:46,379 >> Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 07:19:46 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 07:19:46 - INFO - __main__ -     epoch = 1.0\n",
            "02/10/2021 07:19:46 - INFO - __main__ -     train_runtime = 161.1589\n",
            "02/10/2021 07:19:46 - INFO - __main__ -     train_samples_per_second = 1.52\n",
            "02/10/2021 07:19:46 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-02-10 07:19:46,453 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-02-10 07:19:46,453 >>   Num examples = 13\n",
            "[INFO|trainer.py:1602] 2021-02-10 07:19:46,454 >>   Batch size = 8\n",
            "100% 2/2 [00:01<00:00,  1.81it/s]\n",
            "02/10/2021 07:19:49 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 07:19:49 - INFO - __main__ -     eval_loss = 3.5965094566345215\n",
            "02/10/2021 07:19:49 - INFO - __main__ -     perplexity = 36.4707094145538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2rTWUVcyiKy",
        "outputId": "b04d5910-17e1-459d-cb3f-0c50d41897cb"
      },
      "source": [
        "#Fine tuning GPT2 model at Epoch 3 and learning rate 5e-5\n",
        "!rm -rf /content/output\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 3 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 14:38:53.914016: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/10/2021 14:38:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 14:38:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_14-38-55_8dfc5d5c2fa7, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-b87af0f18388f5b1/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 14:38:56,178 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "02/10/2021 14:38:56 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 14:38:56,178 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 14:38:56 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 14:38:56,448 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "02/10/2021 14:38:56 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 14:38:56,449 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 14:38:56 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:38:57,282 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 14:38:57 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:38:57,282 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 14:38:57 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:38:57,282 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 14:38:57 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 14:38:57,630 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "02/10/2021 14:38:57 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 14:39:11,667 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 14:39:11 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 14:39:11,667 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 14:39:11 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-b87af0f18388f5b1/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-d501aa59b32d5fff.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-b87af0f18388f5b1/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-a52e8478dea1b50e.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-b87af0f18388f5b1/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-2e465b0839133018.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-b87af0f18388f5b1/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-fd0eef341cdd7716.arrow\n",
            "[INFO|trainer.py:432] 2021-02-10 14:39:18,051 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-10 14:39:18,052 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-10 14:39:18,052 >> Using amp fp16 backend\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 14:39:18,053 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 14:39:18 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 14:39:18,058 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 14:39:18 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-10 14:39:18,059 >> ***** Running training *****\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -   ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-10 14:39:18,059 >>   Num examples = 245\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -     Num examples = 245\n",
            "[INFO|trainer.py:839] 2021-02-10 14:39:18,059 >>   Num Epochs = 3\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "[INFO|trainer.py:840] 2021-02-10 14:39:18,059 >>   Instantaneous batch size per device = 8\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-10 14:39:18,059 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 14:39:18,059 >>   Gradient Accumulation steps = 1\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-10 14:39:18,059 >>   Total optimization steps = 735\n",
            "02/10/2021 14:39:18 - INFO - transformers.trainer -     Total optimization steps = 735\n",
            "[WARNING|training_args.py:514] 2021-02-10 14:39:18,065 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 14:39:18 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/735 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 3.6349, 'learning_rate': 1.5986394557823133e-05, 'epoch': 2.04}\n",
            "100% 735/735 [03:05<00:00,  3.98it/s][INFO|trainer.py:1011] 2021-02-10 14:42:23,367 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "02/10/2021 14:42:23 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 185.3081, 'train_samples_per_second': 3.966, 'epoch': 3.0}\n",
            "100% 735/735 [03:05<00:00,  3.97it/s]\n",
            "[INFO|trainer.py:1412] 2021-02-10 14:42:23,369 >> Saving model checkpoint to output\n",
            "02/10/2021 14:42:23 - INFO - transformers.trainer -   Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-02-10 14:42:23,372 >> Configuration saved in output/config.json\n",
            "02/10/2021 14:42:23 - INFO - transformers.configuration_utils -   Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 14:42:31,778 >> Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 14:42:31 - INFO - transformers.modeling_utils -   Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 14:42:31 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 14:42:31 - INFO - __main__ -     epoch = 3.0\n",
            "02/10/2021 14:42:31 - INFO - __main__ -     train_runtime = 185.3081\n",
            "02/10/2021 14:42:31 - INFO - __main__ -     train_samples_per_second = 3.966\n",
            "02/10/2021 14:42:31 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1614] 2021-02-10 14:42:31,888 >> ***** Running Evaluation *****\n",
            "02/10/2021 14:42:31 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1615] 2021-02-10 14:42:31,888 >>   Num examples = 13\n",
            "02/10/2021 14:42:31 - INFO - transformers.trainer -     Num examples = 13\n",
            "[INFO|trainer.py:1616] 2021-02-10 14:42:31,889 >>   Batch size = 8\n",
            "02/10/2021 14:42:31 - INFO - transformers.trainer -     Batch size = 8\n",
            "100% 2/2 [00:00<00:00,  3.35it/s]\n",
            "02/10/2021 14:42:33 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 14:42:33 - INFO - __main__ -     eval_loss = 3.5841269493103027\n",
            "02/10/2021 14:42:33 - INFO - __main__ -     perplexity = 36.02189504628933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zHqE2kXyi8-",
        "outputId": "72ac3fb1-a075-49ad-c26c-23210923bc9b"
      },
      "source": [
        "#Fine tuning GPT2 model at Epoch 5 and learning rate 5e-5\n",
        "!rm -rf /content/output\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 07:29:42.552234: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/10/2021 07:29:43 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 07:29:43 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_07-29-43_d646f54afccc, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 07:29:44,077 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 07:29:44,078 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 07:29:44,129 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 07:29:44,130 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:29:44,300 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:29:44,300 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:29:44,300 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 07:29:44,416 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 07:29:56,561 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 07:29:56,562 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-71d065b5bfa61978.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-1748b7f8decb3fde.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-234b95436ddd724a.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-7e20f59b0cd81916.arrow\n",
            "[INFO|trainer.py:432] 2021-02-10 07:30:00,976 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-10 07:30:00,976 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-10 07:30:00,977 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:30:00,977 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:30:00,982 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-10 07:30:00,982 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-10 07:30:00,982 >>   Num examples = 245\n",
            "[INFO|trainer.py:839] 2021-02-10 07:30:00,982 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:840] 2021-02-10 07:30:00,982 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-10 07:30:00,982 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 07:30:00,982 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-10 07:30:00,983 >>   Total optimization steps = 1225\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:30:00,988 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/1225 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 3.6241, 'learning_rate': 2.959183673469388e-05, 'epoch': 2.04}\n",
            "{'loss': 3.1059, 'learning_rate': 9.183673469387756e-06, 'epoch': 4.08}\n",
            "100% 1225/1225 [13:26<00:00,  1.52it/s][INFO|trainer.py:1007] 2021-02-10 07:43:27,182 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 806.1996, 'train_samples_per_second': 1.519, 'epoch': 5.0}\n",
            "100% 1225/1225 [13:26<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:1408] 2021-02-10 07:43:27,184 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-02-10 07:43:27,186 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 07:43:33,147 >> Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 07:43:33 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 07:43:33 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 07:43:33 - INFO - __main__ -     train_runtime = 806.1996\n",
            "02/10/2021 07:43:33 - INFO - __main__ -     train_samples_per_second = 1.519\n",
            "02/10/2021 07:43:33 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-02-10 07:43:33,245 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-02-10 07:43:33,245 >>   Num examples = 13\n",
            "[INFO|trainer.py:1602] 2021-02-10 07:43:33,246 >>   Batch size = 8\n",
            "100% 2/2 [00:01<00:00,  1.80it/s]\n",
            "02/10/2021 07:43:35 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 07:43:35 - INFO - __main__ -     eval_loss = 3.6389758586883545\n",
            "02/10/2021 07:43:35 - INFO - __main__ -     perplexity = 38.052845271559484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI-6suyDyjzJ",
        "outputId": "11b29e45-d29b-48f2-c585-2a7f95c0f6b9"
      },
      "source": [
        "#Fine tuning GPT2 model at Epoch 7 and learning rate 5e-5\n",
        "!rm -rf /content/output\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 7 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 07:43:38.561924: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/10/2021 07:43:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 07:43:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=7.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_07-43-39_d646f54afccc, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 07:43:40,229 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 07:43:40,229 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 07:43:40,280 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 07:43:40,281 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:43:40,453 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:43:40,453 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 07:43:40,453 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 07:43:40,568 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 07:43:52,924 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 07:43:52,924 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-71d065b5bfa61978.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-1748b7f8decb3fde.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-234b95436ddd724a.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-7e20f59b0cd81916.arrow\n",
            "[INFO|trainer.py:432] 2021-02-10 07:43:57,442 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-10 07:43:57,442 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-10 07:43:57,443 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:43:57,443 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:43:57,448 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-10 07:43:57,448 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-10 07:43:57,448 >>   Num examples = 245\n",
            "[INFO|trainer.py:839] 2021-02-10 07:43:57,449 >>   Num Epochs = 7\n",
            "[INFO|trainer.py:840] 2021-02-10 07:43:57,449 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-10 07:43:57,449 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 07:43:57,449 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-10 07:43:57,449 >>   Total optimization steps = 1715\n",
            "[WARNING|training_args.py:502] 2021-02-10 07:43:57,454 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/1715 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 3.6215, 'learning_rate': 3.542274052478135e-05, 'epoch': 2.04}\n",
            "{'loss': 3.0629, 'learning_rate': 2.0845481049562683e-05, 'epoch': 4.08}\n",
            "{'loss': 2.734, 'learning_rate': 6.268221574344024e-06, 'epoch': 6.12}\n",
            "100% 1715/1715 [18:48<00:00,  1.52it/s][INFO|trainer.py:1007] 2021-02-10 08:02:45,532 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1128.0835, 'train_samples_per_second': 1.52, 'epoch': 7.0}\n",
            "100% 1715/1715 [18:48<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:1408] 2021-02-10 08:02:45,534 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-02-10 08:02:45,536 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 08:02:51,652 >> Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 08:02:51 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 08:02:51 - INFO - __main__ -     epoch = 7.0\n",
            "02/10/2021 08:02:51 - INFO - __main__ -     train_runtime = 1128.0835\n",
            "02/10/2021 08:02:51 - INFO - __main__ -     train_samples_per_second = 1.52\n",
            "02/10/2021 08:02:51 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-02-10 08:02:51,743 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-02-10 08:02:51,743 >>   Num examples = 13\n",
            "[INFO|trainer.py:1602] 2021-02-10 08:02:51,744 >>   Batch size = 8\n",
            "100% 2/2 [00:01<00:00,  1.80it/s]\n",
            "02/10/2021 08:02:54 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 08:02:54 - INFO - __main__ -     eval_loss = 3.7558693885803223\n",
            "02/10/2021 08:02:54 - INFO - __main__ -     perplexity = 42.77138860796756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVbhHQKk7SzU",
        "outputId": "d910d3b7-ffd8-4b41-df7f-3ccc1fea7c77"
      },
      "source": [
        "#Fine tuning GPT2 model at Epoch 5 and learning rate 1e-5\n",
        "!rm -rf /content/output\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--learning_rate 1e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 08:02:59.400614: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/10/2021 08:03:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 08:03:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_08-03-00_d646f54afccc, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 08:03:00,855 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 08:03:00,855 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 08:03:00,904 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 08:03:00,905 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:03:01,074 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:03:01,074 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:03:01,074 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 08:03:01,188 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 08:03:13,264 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 08:03:13,264 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-71d065b5bfa61978.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-1748b7f8decb3fde.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-234b95436ddd724a.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-7e20f59b0cd81916.arrow\n",
            "[INFO|trainer.py:432] 2021-02-10 08:03:17,701 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-10 08:03:17,701 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-10 08:03:17,701 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:03:17,702 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:03:17,707 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-10 08:03:17,707 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-10 08:03:17,707 >>   Num examples = 245\n",
            "[INFO|trainer.py:839] 2021-02-10 08:03:17,707 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:840] 2021-02-10 08:03:17,707 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-10 08:03:17,707 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 08:03:17,707 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-10 08:03:17,707 >>   Total optimization steps = 1225\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:03:17,713 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/1225 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 3.8213, 'learning_rate': 5.918367346938776e-06, 'epoch': 2.04}\n",
            "{'loss': 3.6274, 'learning_rate': 1.8367346938775512e-06, 'epoch': 4.08}\n",
            "100% 1225/1225 [13:25<00:00,  1.52it/s][INFO|trainer.py:1007] 2021-02-10 08:16:43,227 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 805.5194, 'train_samples_per_second': 1.521, 'epoch': 5.0}\n",
            "100% 1225/1225 [13:25<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:1408] 2021-02-10 08:16:43,228 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-02-10 08:16:43,230 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 08:16:49,749 >> Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 08:16:49 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 08:16:49 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 08:16:49 - INFO - __main__ -     train_runtime = 805.5194\n",
            "02/10/2021 08:16:49 - INFO - __main__ -     train_samples_per_second = 1.521\n",
            "02/10/2021 08:16:49 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-02-10 08:16:49,837 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-02-10 08:16:49,837 >>   Num examples = 13\n",
            "[INFO|trainer.py:1602] 2021-02-10 08:16:49,838 >>   Batch size = 8\n",
            "100% 2/2 [00:01<00:00,  1.80it/s]\n",
            "02/10/2021 08:16:52 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 08:16:52 - INFO - __main__ -     eval_loss = 3.6028060913085938\n",
            "02/10/2021 08:16:52 - INFO - __main__ -     perplexity = 36.70107665609659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqldlZpY7Xzz",
        "outputId": "e297f990-06d6-432f-f31c-f2525af6248b"
      },
      "source": [
        "#Fine tuning GPT2 model at Epoch 5 and learning rate 6e-5\n",
        "!rm -rf /content/output\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--learning_rate 6e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 08:16:55.018839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/10/2021 08:16:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 08:16:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=4e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_08-16-56_d646f54afccc, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 08:16:56,510 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 08:16:56,511 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 08:16:56,659 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 08:16:56,660 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:16:56,828 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:16:56,829 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:16:56,829 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 08:16:56,942 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 08:17:09,000 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 08:17:09,000 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-71d065b5bfa61978.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-1748b7f8decb3fde.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-234b95436ddd724a.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-7e20f59b0cd81916.arrow\n",
            "[INFO|trainer.py:432] 2021-02-10 08:17:13,459 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-10 08:17:13,460 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-10 08:17:13,460 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:17:13,460 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:17:13,465 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-10 08:17:13,466 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-10 08:17:13,466 >>   Num examples = 245\n",
            "[INFO|trainer.py:839] 2021-02-10 08:17:13,466 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:840] 2021-02-10 08:17:13,466 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-10 08:17:13,466 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 08:17:13,466 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-10 08:17:13,466 >>   Total optimization steps = 1225\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:17:13,471 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/1225 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 3.6568, 'learning_rate': 2.3673469387755103e-05, 'epoch': 2.04}\n",
            "{'loss': 3.2188, 'learning_rate': 7.346938775510205e-06, 'epoch': 4.08}\n",
            "100% 1225/1225 [13:25<00:00,  1.53it/s][INFO|trainer.py:1007] 2021-02-10 08:30:39,336 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 805.8706, 'train_samples_per_second': 1.52, 'epoch': 5.0}\n",
            "100% 1225/1225 [13:25<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:1408] 2021-02-10 08:30:39,338 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-02-10 08:30:39,340 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 08:30:46,373 >> Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 08:30:46 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 08:30:46 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 08:30:46 - INFO - __main__ -     train_runtime = 805.8706\n",
            "02/10/2021 08:30:46 - INFO - __main__ -     train_samples_per_second = 1.52\n",
            "02/10/2021 08:30:46 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-02-10 08:30:46,459 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-02-10 08:30:46,459 >>   Num examples = 13\n",
            "[INFO|trainer.py:1602] 2021-02-10 08:30:46,460 >>   Batch size = 8\n",
            "100% 2/2 [00:01<00:00,  1.80it/s]\n",
            "02/10/2021 08:30:48 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 08:30:48 - INFO - __main__ -     eval_loss = 3.6082189083099365\n",
            "02/10/2021 08:30:48 - INFO - __main__ -     perplexity = 36.90027148402018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jpa_dud7g-P",
        "outputId": "630cabd6-4c34-40f0-f8d4-b0703025abe2"
      },
      "source": [
        "#Fine tuning GPT2 model at Epoch 5 and learning rate 4e-5\n",
        "!rm -rf /content/output\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 08:30:51.683622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/10/2021 08:30:52 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 08:30:52 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=4e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_08-30-52_d646f54afccc, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 08:30:53,231 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 08:30:53,231 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-10 08:30:53,280 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-10 08:30:53,281 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:30:53,454 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:30:53,454 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 08:30:53,454 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 08:30:53,570 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 08:31:05,678 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 08:31:05,678 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-71d065b5bfa61978.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-1748b7f8decb3fde.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-234b95436ddd724a.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c18b101ca8952b0d/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-7e20f59b0cd81916.arrow\n",
            "[INFO|trainer.py:432] 2021-02-10 08:31:10,190 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-10 08:31:10,191 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-10 08:31:10,191 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:31:10,192 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:31:10,197 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-10 08:31:10,197 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-10 08:31:10,197 >>   Num examples = 245\n",
            "[INFO|trainer.py:839] 2021-02-10 08:31:10,197 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:840] 2021-02-10 08:31:10,197 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-10 08:31:10,197 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 08:31:10,197 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-10 08:31:10,197 >>   Total optimization steps = 1225\n",
            "[WARNING|training_args.py:502] 2021-02-10 08:31:10,203 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/1225 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 3.6568, 'learning_rate': 2.3673469387755103e-05, 'epoch': 2.04}\n",
            "{'loss': 3.2188, 'learning_rate': 7.346938775510205e-06, 'epoch': 4.08}\n",
            "100% 1225/1225 [13:26<00:00,  1.52it/s][INFO|trainer.py:1007] 2021-02-10 08:44:37,005 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 806.8084, 'train_samples_per_second': 1.518, 'epoch': 5.0}\n",
            "100% 1225/1225 [13:26<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:1408] 2021-02-10 08:44:37,007 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-02-10 08:44:37,009 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 08:44:43,907 >> Model weights saved in output/pytorch_model.bin\n",
            "02/10/2021 08:44:43 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 08:44:43 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 08:44:43 - INFO - __main__ -     train_runtime = 806.8084\n",
            "02/10/2021 08:44:44 - INFO - __main__ -     train_samples_per_second = 1.518\n",
            "02/10/2021 08:44:44 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-02-10 08:44:44,000 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-02-10 08:44:44,000 >>   Num examples = 13\n",
            "[INFO|trainer.py:1602] 2021-02-10 08:44:44,001 >>   Batch size = 8\n",
            "100% 2/2 [00:01<00:00,  1.80it/s]\n",
            "02/10/2021 08:44:46 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 08:44:46 - INFO - __main__ -     eval_loss = 3.6082189083099365\n",
            "02/10/2021 08:44:46 - INFO - __main__ -     perplexity = 36.90027148402018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CpzI5mU1jPl"
      },
      "source": [
        "# **Using the model**\n",
        "Generating the text using fine tuned of GPT2 on tiny_shakespeare data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFOx9AUa1tnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b664ba69-91c3-4dd1-9987-a49b12635f4c"
      },
      "source": [
        "# setup imports to use the model\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"/content/output\", from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.17.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'lm_head.weight', 'transformer.h.8.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xT0tc07_-SL"
      },
      "source": [
        "input_ids = tokenizer.encode(\"Hi, How are you?\", return_tensors='tf')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ3YNTNlBsh8"
      },
      "source": [
        "Now we will generate text based on the model that we trained and the initial text that we gave. \"Hi, How are you?\". We returned 5 sequences of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbzHNvvaAPns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dee0b52-6fc9-487f-a7a0-cbdefafb31b8"
      },
      "source": [
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=150,  \n",
        "    num_return_sequences=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPdteSR_B3w1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad494839-9131-46e2-d1c5-7117bd1620bd"
      },
      "source": [
        "#Print output for each sequence generated above\n",
        "for i, beam in enumerate(beam_output):\n",
        "  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n",
        "  print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: Hi, How are you?Come on up. You want to see something about ourselves that we may be in touch with the people there?HENRY BOLINGBROKE:I am much sorry I must not come by now;but they have long since been dissolved and all but forgot us as masters of them.QUEENS OF YORK:Then let me go call for my daughter's brother at her houseThat shall serve 'gainst him a jester-guarder from some other townWhom he is bound by oath to fetch out ifHe should fail his vow unto do so either upon this place or elsewhere.DUCHESS MARGARET CAPULET :Good night! how can any good lodging fare here\n",
            "\n",
            "1: Hi, How are you?If I might tell your lordship of my lifeI would not have the means to perform it butFor having heard how 'twere bornYou did more than one deed upon this earth.MENENIO:What! and is't so great a thing as thatA man hath done what he loved bestAnd loves most honourably now; or in his friends' namesBy whomsoever such love was given him withoutblushing for doing wrongTo any living person's worthy within sight and hearing?Nay, if even thou shouldst be toldHow often an angel fell by thy own handIn our midst from heaven on theeWhose face with weeping eyes met mine--and then said:'O wretched soul!'My\n",
            "\n",
            "2: Hi, How are you? My wife's coming home.I'll have my maid and a dog; I come to see how they do:My lord of Warwick hath sent me word that one is gone from usWith him at sea or in another vessel with lossOf some soldier for York.'QUEEN MARGARET:''Twas the Lord Hastings,' quoth his son Edward ere longThat he had lost Henry Duke Of Lancaster himselfAnd three thousand soldiers as wellAs twenty horses by their master slainIn battle about Tybalt Shore--and now we may discoverWhether it were so!Lord Angelo dined last night near Richmond fieldUpon what news arrived hereAt Derby yesterday morning which makes such discontentA great degree between myself And\n",
            "\n",
            "3: Hi, How are you?JOHN:O good gracious man!KING RICHARD III.Good saint Richard of York is dead at Exeter castle; and his head I will bring back to France where he shall be buried in the churchyard aforesaid with sweet intercession made for him by all God's friends here belowAnd that from this point forward we may never fear what Rome or English king dare doTo cross our sea-coast nor drive us off your shoresOr break their holy peace betimes while thou art warmly greeting themWithin sight o' the royal palace now set upThy father Henry being slain there on highAbove which was a great statue wherein thy mother sitsWith many fair flowers above her crownWhich she would have borne\n",
            "\n",
            "4: Hi, How are you?Will ye marry me for a monthAnd let your love melt into tears by the time of our day?Now we were both in grief to-day and needfulI wanton'd something: I had rather sit down than stand up;and then lie weeping. Good heavens!LADY ANNE'S MOTHERDUKE ROSSUS MARCIUMMENENSISRATIO's motherhood is so strongA man cannot get it off with words but he must doIt himself--if that be possible at all without cryingAfter his son or father has died!--then call upon him againWhen life begins on earth till death does usurpThat which was once yours before thee born too dearTo us\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUl3SCGEW3q"
      },
      "source": [
        "# **Fine tuning GPT2 model on Bill Sum data set** italicized text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh3b6vtzEVRc"
      },
      "source": [
        "Defining a funtion for loading the data set ( bill_sum) and converting it to text an saving the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTHLZ30ee39i"
      },
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "def to_txt_save(split_name):\n",
        "  df = pd.DataFrame(tfds.load(name='billsum')[split_name])\n",
        "  data = df['text'].tolist()\n",
        "  final = ''\n",
        "  for text in data:\n",
        "    text = text.numpy().decode(\"utf-8\")\n",
        "    final += text\n",
        "  with open(split_name+'_bill'+'.txt', 'w') as f:\n",
        "    f.write(final)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JjeIUe-kdIe",
        "outputId": "ece33185-307b-47c7-fc0d-db2537c674e3"
      },
      "source": [
        "# import pandas as pd\n",
        "# df = pd.DataFrame(tfds.load(name='billsum')['train'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/billsum/3.0.0\n",
            "INFO:absl:Reusing dataset billsum (/root/tensorflow_datasets/billsum/3.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/billsum/3.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln-DttWEkmbS",
        "outputId": "e1b7a5db-5dc4-4cc0-992d-28c583407148"
      },
      "source": [
        "#  print(df['text'][900].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SECTION 1. SHORT TITLE.\n",
            "\n",
            "    This Act may be cited as the ``California Ocean Protection Act of \n",
            "1995''.\n",
            "\n",
            "SEC. 2. FINDINGS.\n",
            "\n",
            "    Congress finds that--\n",
            "            (1) the coast of California possesses unique historical, \n",
            "        ecological, educational, recreational, economic, and research \n",
            "        values that are appropriate for protection under Federal law;\n",
            "            (2) the threat to the coast of California, a national \n",
            "        treasure, continues to intensify as a result of fossil fuel \n",
            "        exploration and development, mineral extraction, and the \n",
            "        burning and dumping of toxic and hazardous wastes;\n",
            "            (3) the activities described in paragraph (2) could result \n",
            "        in irreparable damage to the coast of California; and\n",
            "            (4) the establishment of an ocean protection zone off the \n",
            "        coast of California would enhance recreational and commercial \n",
            "        fisheries, and the use of renewable resources within the zone.\n",
            "\n",
            "SEC. 3. DEFINITIONS.\n",
            "\n",
            "    In this Act:\n",
            "            (1) Administrator.--The term ``Administrator'' means the \n",
            "        Administrator of the Environmental Protection Agency.\n",
            "            (2) Development.--The term ``development'' has the meaning \n",
            "        stated in section 2 of the Outer Continental Shelf Lands Act \n",
            "        (43 U.S.C. 1331).\n",
            "            (3) Exclusive economic zone.--The term ``Exclusive Economic \n",
            "        Zone'' means the Exclusive Economic Zone of the United States, \n",
            "        as defined by Presidential Proclamation 5030 of March 10, 1983.\n",
            "            (4) Exploration.--The term ``exploration'' has the meaning \n",
            "        stated in section 2 of the Outer Continental Shelf Lands Act \n",
            "        (43 U.S.C. 1331).\n",
            "            (5) Harmful ocean dumping.--The term ``harmful ocean \n",
            "        dumping''--\n",
            "                    (A) shall have the meaning provided by the \n",
            "                Administrator, in consultation with the heads of other \n",
            "                Federal agencies whom the Administrator determines to \n",
            "                be appropriate; but\n",
            "                    (B) shall not include--\n",
            "                            (i) a de minimus disposal of vessel waste;\n",
            "                            (ii) the disposal of dredged material \n",
            "                        that--\n",
            "                                    (I) would meet the requirements for \n",
            "                                disposal under the criteria established \n",
            "                                under section 103 of the Marine \n",
            "                                Protection, Research, and Sanctuaries \n",
            "                                Act of 1972 (33 U.S.C. 1413), including \n",
            "                                regulations promulgated under that \n",
            "                                section; or\n",
            "                                    (II) is disposed of pursuant to a \n",
            "                                permit issued pursuant to that section;\n",
            "                            (iii) a discharge that is authorized under \n",
            "                        a National Pollutant Discharge Elimination \n",
            "                        System (NPDES) permit issued under section 402 \n",
            "                        of the Federal Water Pollution Control Act (33 \n",
            "                        U.S.C. 1342); or\n",
            "                            (iv) a disposal that is carried out by an \n",
            "                        appropriate Federal agency under title I of the \n",
            "                        Marine Protection, Research, and Sanctuaries \n",
            "                        Act of 1972 (33 U.S.C. 1411 et seq.).\n",
            "            (6) Minerals.--The term ``minerals'' has the meaning stated \n",
            "        in section 2 of the Outer Continental Shelf Lands Act (43 \n",
            "        U.S.C. 1331).\n",
            "            (7) Outer continental shelf.--The term ``outer Continental \n",
            "        Shelf'' has the meaning stated in section 2 of the Outer \n",
            "        Continental Shelf Lands Act (43 U.S.C. 1331).\n",
            "            (8) Person.--The term ``person'' has the meaning stated in \n",
            "        section 2 of the Outer Continental Shelf Lands Act (43 U.S.C. \n",
            "        1331).\n",
            "            (9) Production.--The term ``production'' has the meaning \n",
            "        stated in section 2 of such Act (43 U.S.C. 1331).\n",
            "            (10) Territorial sea .--The term ``territorial sea'' means \n",
            "        the belt of sea measured from the baseline of the United \n",
            "        States, determined in accordance with international law, as set \n",
            "        forth in Presidential Proclamation 5928, dated December 27, \n",
            "        1988.\n",
            "            (11) Zone.--The term ``Zone'' means the California Ocean \n",
            "        Protection Zone established under section 4.\n",
            "\n",
            "SEC. 4. DESIGNATION OF CALIFORNIA OCEAN PROTECTION ZONE.\n",
            "\n",
            "    There is established a California Ocean Protection Zone, consisting \n",
            "of--\n",
            "            (1) waters of the Exclusive Economic Zone that are \n",
            "        contiguous to the waters of the territorial sea that are \n",
            "        contiguous to the State of California;\n",
            "            (2) waters of the territorial sea that are contiguous to \n",
            "        the State of California; and\n",
            "            (3) the portion of the outer Continental Shelf underlying \n",
            "        those waters.\n",
            "SEC. 5. RESTRICTIONS.\n",
            "\n",
            "    (a) Mineral Exploration, Development, and Production.--\n",
            "            (1) Definition.--In this subsection, the term ``lease'' has \n",
            "        the meaning stated in section 2 of the Outer Continental Shelf \n",
            "        Lands Act (43 U.S.C. 1331).\n",
            "            (2) Issuance of leases, permits, and licenses.--\n",
            "        Notwithstanding any other law, the head of a Federal agency may \n",
            "        not issue a lease, permit, or license for the exploration for \n",
            "        or development or production of oil, gas, or other minerals in \n",
            "        or from the Zone.\n",
            "            (3) Exploration, development, and production.--\n",
            "        Notwithstanding any other law, a person may not engage in the \n",
            "        exploration for, or the development or production of, oil, gas, \n",
            "        or other minerals in or from the Zone after the date of the \n",
            "        cancellation, expiration, relinquishment, or termination of a \n",
            "        lease, permit, or license in effect on June ____, 1995, that \n",
            "        permits exploration, development, or production.\n",
            "    (b) Ocean Incineration and Dumping.--Notwithstanding any other law, \n",
            "the head of a Federal agency may not issue a lease, permit, or license \n",
            "for--\n",
            "            (1) ocean incineration or harmful ocean dumping within the \n",
            "        Zone; or\n",
            "            (2) any onshore facility that facilitates ocean \n",
            "        incineration or harmful ocean dumping within the Zone.\n",
            "\n",
            "SEC. 6. FISHING.\n",
            "\n",
            "    This Act is not intended to regulate, restrict, or prohibit \n",
            "commercial or recreational fishing, or other harvesting of ocean life \n",
            "in the Zone.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvzKoM3bfHV8",
        "outputId": "e1e57582-5e9e-435d-9565-ee4f39899dfa"
      },
      "source": [
        "to_txt_save('train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/billsum/3.0.0\n",
            "INFO:absl:Reusing dataset billsum (/root/tensorflow_datasets/billsum/3.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/billsum/3.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgbYZIlWgf6x",
        "outputId": "ad19c5d9-5d06-4427-996a-2a0bb84765eb"
      },
      "source": [
        "to_txt_save('test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/billsum/3.0.0\n",
            "INFO:absl:Reusing dataset billsum (/root/tensorflow_datasets/billsum/3.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/billsum/3.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXrWKTKjpf9d"
      },
      "source": [
        "\n",
        "!rm -rf /content/output_bill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieftptA6fS3J",
        "outputId": "5567a7fa-f6be-4c3b-a88c-72f6d6a86a81"
      },
      "source": [
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file /content/train_bill.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/test_bill.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--fp16 \\\n",
        "--output_dir=output_bill"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-06 14:47:19.720236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/06/2021 14:47:21 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/06/2021 14:47:21 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output_bill, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb06_14-47-21_91d59f600620, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output_bill, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-76c543b737bc581f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:449] 2021-02-06 14:47:22,121 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-06 14:47:22,122 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-02-06 14:47:22,390 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:485] 2021-02-06 14:47:22,391 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-06 14:47:23,217 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-06 14:47:23,217 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-06 14:47:23,217 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-06 14:47:23,562 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-06 14:47:37,526 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-06 14:47:37,526 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-76c543b737bc581f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-9fa2e83c229f8868.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-76c543b737bc581f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-819c4fa1ca2fa7bf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-76c543b737bc581f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-90fb8e395eb563b9.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-76c543b737bc581f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-d98a602aae682eb6.arrow\n",
            "[INFO|trainer.py:432] 2021-02-06 14:47:43,752 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:432] 2021-02-06 14:47:43,753 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:348] 2021-02-06 14:47:43,753 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:502] 2021-02-06 14:47:43,754 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:502] 2021-02-06 14:47:43,759 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:837] 2021-02-06 14:47:43,760 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-02-06 14:47:43,760 >>   Num examples = 62913\n",
            "[INFO|trainer.py:839] 2021-02-06 14:47:43,760 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:840] 2021-02-06 14:47:43,760 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:841] 2021-02-06 14:47:43,760 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:842] 2021-02-06 14:47:43,760 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-02-06 14:47:43,760 >>   Total optimization steps = 62913\n",
            "[WARNING|training_args.py:502] 2021-02-06 14:47:43,766 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/62913 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 1.1443, 'learning_rate': 4.960262584839381e-05, 'epoch': 0.01}\n",
            "{'loss': 1.0595, 'learning_rate': 4.920525169678763e-05, 'epoch': 0.02}\n",
            "{'loss': 1.0189, 'learning_rate': 4.880787754518144e-05, 'epoch': 0.02}\n",
            "{'loss': 1.0178, 'learning_rate': 4.8410503393575256e-05, 'epoch': 0.03}\n",
            "{'loss': 1.0014, 'learning_rate': 4.8013129241969065e-05, 'epoch': 0.04}\n",
            "{'loss': 0.9908, 'learning_rate': 4.761575509036289e-05, 'epoch': 0.05}\n",
            "{'loss': 0.9899, 'learning_rate': 4.7218380938756695e-05, 'epoch': 0.06}\n",
            "{'loss': 0.9783, 'learning_rate': 4.682100678715051e-05, 'epoch': 0.06}\n",
            "{'loss': 0.9732, 'learning_rate': 4.6423632635544326e-05, 'epoch': 0.07}\n",
            "{'loss': 0.9782, 'learning_rate': 4.602625848393814e-05, 'epoch': 0.08}\n",
            "{'loss': 0.9729, 'learning_rate': 4.562888433233195e-05, 'epoch': 0.09}\n",
            "{'loss': 0.9308, 'learning_rate': 4.5231510180725765e-05, 'epoch': 0.1}\n",
            "{'loss': 0.9446, 'learning_rate': 4.483413602911958e-05, 'epoch': 0.1}\n",
            "{'loss': 0.9327, 'learning_rate': 4.4436761877513395e-05, 'epoch': 0.11}\n",
            "{'loss': 0.9471, 'learning_rate': 4.4039387725907204e-05, 'epoch': 0.12}\n",
            "{'loss': 0.9471, 'learning_rate': 4.364201357430102e-05, 'epoch': 0.13}\n",
            "{'loss': 0.9229, 'learning_rate': 4.3244639422694834e-05, 'epoch': 0.14}\n",
            "{'loss': 0.9316, 'learning_rate': 4.284726527108865e-05, 'epoch': 0.14}\n",
            "{'loss': 0.9318, 'learning_rate': 4.244989111948246e-05, 'epoch': 0.15}\n",
            "{'loss': 0.9094, 'learning_rate': 4.205251696787628e-05, 'epoch': 0.16}\n",
            "{'loss': 0.9175, 'learning_rate': 4.165514281627009e-05, 'epoch': 0.17}\n",
            "{'loss': 0.9014, 'learning_rate': 4.12577686646639e-05, 'epoch': 0.17}\n",
            "{'loss': 0.8911, 'learning_rate': 4.086039451305771e-05, 'epoch': 0.18}\n",
            "{'loss': 0.9099, 'learning_rate': 4.0463020361451534e-05, 'epoch': 0.19}\n",
            "{'loss': 0.8867, 'learning_rate': 4.006564620984534e-05, 'epoch': 0.2}\n",
            "{'loss': 0.8906, 'learning_rate': 3.966827205823916e-05, 'epoch': 0.21}\n",
            "{'loss': 0.8829, 'learning_rate': 3.927089790663297e-05, 'epoch': 0.21}\n",
            "{'loss': 0.8669, 'learning_rate': 3.887352375502679e-05, 'epoch': 0.22}\n",
            "{'loss': 0.9189, 'learning_rate': 3.8476149603420596e-05, 'epoch': 0.23}\n",
            "{'loss': 0.9022, 'learning_rate': 3.807877545181441e-05, 'epoch': 0.24}\n",
            "{'loss': 0.8741, 'learning_rate': 3.768140130020823e-05, 'epoch': 0.25}\n",
            "{'loss': 0.9107, 'learning_rate': 3.728402714860204e-05, 'epoch': 0.25}\n",
            "{'loss': 0.8812, 'learning_rate': 3.688665299699585e-05, 'epoch': 0.26}\n",
            "{'loss': 0.8787, 'learning_rate': 3.6489278845389666e-05, 'epoch': 0.27}\n",
            "{'loss': 0.8776, 'learning_rate': 3.609190469378348e-05, 'epoch': 0.28}\n",
            "{'loss': 0.8818, 'learning_rate': 3.5694530542177296e-05, 'epoch': 0.29}\n",
            "{'loss': 0.8656, 'learning_rate': 3.5297156390571105e-05, 'epoch': 0.29}\n",
            "{'loss': 0.8671, 'learning_rate': 3.4899782238964927e-05, 'epoch': 0.3}\n",
            "{'loss': 0.8782, 'learning_rate': 3.4502408087358735e-05, 'epoch': 0.31}\n",
            "{'loss': 0.8471, 'learning_rate': 3.410503393575255e-05, 'epoch': 0.32}\n",
            "{'loss': 0.8892, 'learning_rate': 3.370765978414636e-05, 'epoch': 0.33}\n",
            "{'loss': 0.8877, 'learning_rate': 3.331028563254018e-05, 'epoch': 0.33}\n",
            "{'loss': 0.8649, 'learning_rate': 3.291291148093399e-05, 'epoch': 0.34}\n",
            "{'loss': 0.8626, 'learning_rate': 3.2515537329327804e-05, 'epoch': 0.35}\n",
            "{'loss': 0.8694, 'learning_rate': 3.211816317772162e-05, 'epoch': 0.36}\n",
            "{'loss': 0.8652, 'learning_rate': 3.1720789026115435e-05, 'epoch': 0.37}\n",
            "{'loss': 0.8413, 'learning_rate': 3.132341487450924e-05, 'epoch': 0.37}\n",
            "{'loss': 0.8668, 'learning_rate': 3.092604072290306e-05, 'epoch': 0.38}\n",
            "{'loss': 0.8652, 'learning_rate': 3.0528666571296874e-05, 'epoch': 0.39}\n",
            "{'loss': 0.8501, 'learning_rate': 3.013129241969069e-05, 'epoch': 0.4}\n",
            "{'loss': 0.8755, 'learning_rate': 2.9733918268084497e-05, 'epoch': 0.41}\n",
            "{'loss': 0.8386, 'learning_rate': 2.9336544116478316e-05, 'epoch': 0.41}\n",
            "{'loss': 0.8538, 'learning_rate': 2.8939169964872124e-05, 'epoch': 0.42}\n",
            "{'loss': 0.8662, 'learning_rate': 2.8541795813265943e-05, 'epoch': 0.43}\n",
            "{'loss': 0.8484, 'learning_rate': 2.8144421661659755e-05, 'epoch': 0.44}\n",
            "{'loss': 0.8785, 'learning_rate': 2.774704751005357e-05, 'epoch': 0.45}\n",
            "{'loss': 0.8461, 'learning_rate': 2.7349673358447382e-05, 'epoch': 0.45}\n",
            "{'loss': 0.8357, 'learning_rate': 2.6952299206841197e-05, 'epoch': 0.46}\n",
            "{'loss': 0.8636, 'learning_rate': 2.655492505523501e-05, 'epoch': 0.47}\n",
            "{'loss': 0.8304, 'learning_rate': 2.6157550903628824e-05, 'epoch': 0.48}\n",
            "{'loss': 0.8535, 'learning_rate': 2.5760176752022636e-05, 'epoch': 0.48}\n",
            "{'loss': 0.8304, 'learning_rate': 2.536280260041645e-05, 'epoch': 0.49}\n",
            "{'loss': 0.8553, 'learning_rate': 2.4965428448810263e-05, 'epoch': 0.5}\n",
            "{'loss': 0.8592, 'learning_rate': 2.4568054297204078e-05, 'epoch': 0.51}\n",
            "{'loss': 0.8484, 'learning_rate': 2.417068014559789e-05, 'epoch': 0.52}\n",
            "{'loss': 0.843, 'learning_rate': 2.3773305993991705e-05, 'epoch': 0.52}\n",
            "{'loss': 0.8448, 'learning_rate': 2.3375931842385517e-05, 'epoch': 0.53}\n",
            "{'loss': 0.8342, 'learning_rate': 2.2978557690779332e-05, 'epoch': 0.54}\n",
            "{'loss': 0.8427, 'learning_rate': 2.2581183539173144e-05, 'epoch': 0.55}\n",
            "{'loss': 0.8355, 'learning_rate': 2.218380938756696e-05, 'epoch': 0.56}\n",
            "{'loss': 0.7963, 'learning_rate': 2.178643523596077e-05, 'epoch': 0.56}\n",
            "{'loss': 0.8277, 'learning_rate': 2.1389061084354586e-05, 'epoch': 0.57}\n",
            "{'loss': 0.833, 'learning_rate': 2.09916869327484e-05, 'epoch': 0.58}\n",
            "{'loss': 0.8352, 'learning_rate': 2.0594312781142213e-05, 'epoch': 0.59}\n",
            "{'loss': 0.8293, 'learning_rate': 2.019693862953603e-05, 'epoch': 0.6}\n",
            "{'loss': 0.8306, 'learning_rate': 1.979956447792984e-05, 'epoch': 0.6}\n",
            "{'loss': 0.7947, 'learning_rate': 1.9402190326323656e-05, 'epoch': 0.61}\n",
            "{'loss': 0.8375, 'learning_rate': 1.9004816174717468e-05, 'epoch': 0.62}\n",
            "{'loss': 0.8104, 'learning_rate': 1.8607442023111283e-05, 'epoch': 0.63}\n",
            "{'loss': 0.7996, 'learning_rate': 1.8210067871505095e-05, 'epoch': 0.64}\n",
            "{'loss': 0.8355, 'learning_rate': 1.781269371989891e-05, 'epoch': 0.64}\n",
            "{'loss': 0.8452, 'learning_rate': 1.7415319568292725e-05, 'epoch': 0.65}\n",
            "{'loss': 0.8215, 'learning_rate': 1.7017945416686537e-05, 'epoch': 0.66}\n",
            "{'loss': 0.8296, 'learning_rate': 1.6620571265080352e-05, 'epoch': 0.67}\n",
            "{'loss': 0.8119, 'learning_rate': 1.6223197113474164e-05, 'epoch': 0.68}\n",
            "{'loss': 0.798, 'learning_rate': 1.582582296186798e-05, 'epoch': 0.68}\n",
            "{'loss': 0.8467, 'learning_rate': 1.542844881026179e-05, 'epoch': 0.69}\n",
            "{'loss': 0.8186, 'learning_rate': 1.5031074658655606e-05, 'epoch': 0.7}\n",
            "{'loss': 0.8297, 'learning_rate': 1.463370050704942e-05, 'epoch': 0.71}\n",
            "{'loss': 0.8205, 'learning_rate': 1.4236326355443233e-05, 'epoch': 0.72}\n",
            "{'loss': 0.8049, 'learning_rate': 1.3838952203837047e-05, 'epoch': 0.72}\n",
            "{'loss': 0.822, 'learning_rate': 1.344157805223086e-05, 'epoch': 0.73}\n",
            "{'loss': 0.81, 'learning_rate': 1.3044203900624674e-05, 'epoch': 0.74}\n",
            "{'loss': 0.7898, 'learning_rate': 1.2646829749018487e-05, 'epoch': 0.75}\n",
            "{'loss': 0.8169, 'learning_rate': 1.2249455597412301e-05, 'epoch': 0.76}\n",
            "{'loss': 0.7843, 'learning_rate': 1.1852081445806114e-05, 'epoch': 0.76}\n",
            "{'loss': 0.8029, 'learning_rate': 1.1454707294199928e-05, 'epoch': 0.77}\n",
            "{'loss': 0.8154, 'learning_rate': 1.1057333142593741e-05, 'epoch': 0.78}\n",
            "{'loss': 0.8219, 'learning_rate': 1.0659958990987555e-05, 'epoch': 0.79}\n",
            "{'loss': 0.7948, 'learning_rate': 1.0262584839381368e-05, 'epoch': 0.79}\n",
            "{'loss': 0.8222, 'learning_rate': 9.865210687775182e-06, 'epoch': 0.8}\n",
            "{'loss': 0.8097, 'learning_rate': 9.467836536168996e-06, 'epoch': 0.81}\n",
            "{'loss': 0.7941, 'learning_rate': 9.070462384562809e-06, 'epoch': 0.82}\n",
            "{'loss': 0.8232, 'learning_rate': 8.673088232956624e-06, 'epoch': 0.83}\n",
            "{'loss': 0.7972, 'learning_rate': 8.275714081350438e-06, 'epoch': 0.83}\n",
            "{'loss': 0.7946, 'learning_rate': 7.878339929744251e-06, 'epoch': 0.84}\n",
            "{'loss': 0.795, 'learning_rate': 7.480965778138065e-06, 'epoch': 0.85}\n",
            "{'loss': 0.7883, 'learning_rate': 7.083591626531878e-06, 'epoch': 0.86}\n",
            "{'loss': 0.7897, 'learning_rate': 6.686217474925692e-06, 'epoch': 0.87}\n",
            "{'loss': 0.7912, 'learning_rate': 6.288843323319505e-06, 'epoch': 0.87}\n",
            "{'loss': 0.794, 'learning_rate': 5.891469171713319e-06, 'epoch': 0.88}\n",
            "{'loss': 0.7964, 'learning_rate': 5.4940950201071325e-06, 'epoch': 0.89}\n",
            "{'loss': 0.7951, 'learning_rate': 5.096720868500946e-06, 'epoch': 0.9}\n",
            "{'loss': 0.7935, 'learning_rate': 4.6993467168947595e-06, 'epoch': 0.91}\n",
            "{'loss': 0.7955, 'learning_rate': 4.301972565288574e-06, 'epoch': 0.91}\n",
            "{'loss': 0.8106, 'learning_rate': 3.9045984136823874e-06, 'epoch': 0.92}\n",
            "{'loss': 0.8091, 'learning_rate': 3.507224262076201e-06, 'epoch': 0.93}\n",
            "{'loss': 0.8132, 'learning_rate': 3.1098501104700145e-06, 'epoch': 0.94}\n",
            "{'loss': 0.8, 'learning_rate': 2.712475958863828e-06, 'epoch': 0.95}\n",
            "{'loss': 0.8028, 'learning_rate': 2.3151018072576416e-06, 'epoch': 0.95}\n",
            "{'loss': 0.8108, 'learning_rate': 1.9177276556514555e-06, 'epoch': 0.96}\n",
            "{'loss': 0.7949, 'learning_rate': 1.520353504045269e-06, 'epoch': 0.97}\n",
            "{'loss': 0.7832, 'learning_rate': 1.1229793524390826e-06, 'epoch': 0.98}\n",
            "{'loss': 0.7976, 'learning_rate': 7.256052008328963e-07, 'epoch': 0.99}\n",
            "{'loss': 0.7847, 'learning_rate': 3.282310492267099e-07, 'epoch': 0.99}\n",
            "100% 62913/62913 [4:20:08<00:00,  4.02it/s][INFO|trainer.py:1007] 2021-02-06 19:07:52,150 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 15608.3902, 'train_samples_per_second': 4.031, 'epoch': 1.0}\n",
            "100% 62913/62913 [4:20:08<00:00,  4.03it/s]\n",
            "[INFO|trainer.py:1408] 2021-02-06 19:07:52,152 >> Saving model checkpoint to output_bill\n",
            "[INFO|configuration_utils.py:304] 2021-02-06 19:07:52,154 >> Configuration saved in output_bill/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-06 19:07:57,369 >> Model weights saved in output_bill/pytorch_model.bin\n",
            "02/06/2021 19:07:57 - INFO - __main__ -   ***** Train results *****\n",
            "02/06/2021 19:07:57 - INFO - __main__ -     epoch = 1.0\n",
            "02/06/2021 19:07:57 - INFO - __main__ -     train_runtime = 15608.3902\n",
            "02/06/2021 19:07:57 - INFO - __main__ -     train_samples_per_second = 4.031\n",
            "02/06/2021 19:07:57 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-02-06 19:07:57,456 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-02-06 19:07:57,456 >>   Num examples = 10903\n",
            "[INFO|trainer.py:1602] 2021-02-06 19:07:57,457 >>   Batch size = 8\n",
            "100% 1363/1363 [17:08<00:00,  1.33it/s]\n",
            "02/06/2021 19:25:06 - INFO - __main__ -   ***** Eval results *****\n",
            "02/06/2021 19:25:06 - INFO - __main__ -     perplexity = 2.159936403129259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUwALvhsfeVT",
        "outputId": "950643fd-07e5-4329-d9e6-b3549b9d6a47"
      },
      "source": [
        "#mounting google drive to save the model as it is taking a lot of time to train. So I trained once and saved the model in the drive for future use\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nfqynmDue9Z"
      },
      "source": [
        "#copying the file in the google drive\n",
        "!cp -r /content/output_bill /content/drive/MyDrive/output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5vgsP27hwQn",
        "outputId": "b8337e39-a7d4-42e6-c56d-762be1157b52"
      },
      "source": [
        "ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md         \u001b[0m\u001b[01;32mrun_clm.py\u001b[0m*       \u001b[01;32mrun_mlm.py\u001b[0m*\n",
            "requirements.txt  \u001b[01;32mrun_mlm_flax.py\u001b[0m*  \u001b[01;32mrun_plm.py\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o_z1fdczrEn",
        "outputId": "5610584a-eb11-4915-e739-177aaf1607c8"
      },
      "source": [
        "# setup imports to use the model\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "#loading the model saved in the drive for generating text\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/output\", from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.23.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'lm_head.weight', 'transformer.h.15.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.0.attn.masked_bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD0a_327z_Hv"
      },
      "source": [
        "input_ids = tokenizer.encode(\"I am assuming you are dead.\", return_tensors='tf')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWMMWwXZ0PEh",
        "outputId": "be2caa26-931b-4392-fd3b-a0a27ef080ec"
      },
      "source": [
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=150,  \n",
        "    num_return_sequences=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-sNf1dX0PoR",
        "outputId": "8d22573d-2dd8-4024-93ca-8ce217c40b1b"
      },
      "source": [
        "#Print output for each sequence generated above\n",
        "for i, beam in enumerate(beam_output):\n",
        "  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n",
        "  print()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: I am assuming you are dead. If I have not, the following is true: If you do die during the period of time described in paragraph (1), this Act shall cease to be effective with respect such person for a year and each year thereafter until he or she dies.''.SECTION 1; SHORT TITLE AND FINDINGS FOR PURPOSES OF TITLES II TO THE UNIFORMED SERVICES ACT, 1974, AND THE LANDMARK CIVIL ACTIONS ENFORCEMENT ACT ISSUED BY THE UNITED STATES COURTS.SEC 2 CONGRESSIONAL CONSIDERATION OF AIRCRAFT PLACING IN LANGUAGES OTHER THAN HOST REGARDANT TO PUBLIC SAFETY,\n",
            "\n",
            "1: I am assuming you are dead. ____. A man, woman and child clad in the uniform of the United States Army, may be required to pay a fee for carrying out section 6, which shall not exceed $500. The headings ``Federal'' AND``Medicaid''.SEC 2 INQUIRIES BY THE UNITED STATES ARMY REGARDING THE APPOINTMENT OF OFFICERS FOR COAST GUARDS. (a) In General.--The Secretary concerned, or his designee is authorized by law--(1)(A), may request the Assistant Commandant at any time from the Department's Office with responsibility under this Act that the AssistantCommandant examine all applications for appointment as Coast Guard officers\n",
            "\n",
            "2: I am assuming you are dead. The problem is not that we have no evidence of your existence, but instead that there is no way to prove or disprove the theory that you exist. This is what the conspirators mean when they say `` You will find our evidence by examining [the] papers and records''. In order for this claim--which claims the impossible--to be true a conspiracy must have occurred before June 1st. On July 31st, all those who had been making statements about their plans and workmen's actions in May 1845 can have proven it wrong. No one else has any reason other than us, except as a consequence if he was murdered after his death (according both Federal\n",
            "\n",
            "3: I am assuming you are dead. I was born with no mouth and tons of brain wires, so my speech is completely garbled by now. But if we could only find a way to maintain it for the future, then someone else would have to take care''.SECONDARY SPIKES IN THE CHILDREN OF INDIAN STUDENTS. (a) In General.--Section 9101(6)(A), as amended in subpart B--Not later than 18 months after enactment;by striking ``as required'' each place such term appears until he or she has identified at least one alternate hearing aid described under clause 3 through 5 which any individual may be provided aural facsimile hearing assistance\n",
            "\n",
            "4: I am assuming you are dead. If I were to be proven wrong, then what does this prove? If there is a difference between the claims made in this article and the information provided by other witnesses who claimed that there was no evidence at any time of an alien or the aliens' presence on board the vessel,then proof of such differences would not matter. The American Civil Liberties Union filed a civil suit against the United States after the court ordered the Secretary concerned take all actions necessary for this action as determined under section 3(a). In its civil case before the Supreme Court (United State) it asserted: ``All persons who are alleged injured by this Act shall have standing. They are entitled only if they can assert their injuries from a\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-npUv43e0SlT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpeVjDggvL1x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}